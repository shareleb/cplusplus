# 向量检索

## 什么是向量检索？
  **向量检索其实就是计算向量之间的相似度，返回相似度对高的TopK向量**
## 向量搜索常用的场景有哪些？
  **图像检索，语音识别， 搜广推， NLP**
## 向量搜索中的常用的距离公式有哪些？
  浮点型向量计算公式：
    1. 内积或点积（IP）: 计算的是两个向量在方向上的差异，夹角越小越相似，因此内积值越大越相似。 （a,b) (x,y)  ax + by  = |a||b|cos
                 更适合计算向量的方向而不是大小,  几何意义上是计算一条向量在另一条向量上的垂直投影长度。 
                 如果只想比较方向上的相识度，那么就需要对向量进行归一化。
    2. 欧式（L2） 两点之间最短的直线距离，距离值越小越相似。 （a,b) (x,y)  sqrt[(a-x) * (a-x) + (b-y) * (b-y)]
    3. 余弦（Cosine） 余弦距离计算的是两个向量之间的夹角余弦值，夹角越小越相似，因此余弦相似度值越大越相似  x ,y 两向量 x*y/ |x| |y|
        余弦距离和内积距离更多的是从方向上区分差异，而对绝对的数值不敏感,更多的用于使用用户对内容评分来区分兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题。 归一化后，内积与余弦相似度计算公式等价
  二值型向量计算方式：
    1. 汉明距离 (Hamming) :  汉明距离计算二进制字符串之间的距离。两个等长字符串之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。
        假设有两条字符串 1101 1001 和 1001 1101。比较时，如果字符相同用 0 表示，如果字符不同则用1表示 11011001 ⊕ 10011101 = 01000100汉明距离为 2。
    2. 杰卡德距离 (Jaccard) :
             数据集交集的个数和并集个数的比值。计算公式可以表示为 
            J(a,b) = a 交 b | |a| + |b| - |a交b|  杰卡德距离是用来衡量两个数据集差异性的一种指标 杰卡德距离适合字符串相似性度量。
    3. 谷本距离 (Tanimoto) 。。。。

  向量归一化公式，归一化后 ，向量模长|X’|等于1 

## 向量搜索中常用的库有哪些？
    Faiss  facebook 轻量级，便于集成。
    Milvus ： Milvus 2.0是一个云原生向量数据库，设计上采用存储、计算分离 适合大规模数据集功能丰富。
## 向量搜索中常用的算法有哪些？
   KD树：
        构建：
            选择向量上方差最大的维度，中位数m，作为节点的切分值，小于m的划分到左边，大于m的划分到右边。 同时递归的构建左子树和右子树。
            当集合的数据等于1个元素或者小于某个阈值时停止分裂。
        查询：1. 给定查询元素Q，判断节点的切分维度,Q(n) < node.val 则走左子树，大于等于则走右子树。 
             2. 一直走到叶子节点，计算Q与叶子节点的距离，同时保存为当前最近距离。
             3. 在叶子节点进行回溯操作，每次回溯都判断Q与父节点中未被访问到的分支的距离
                3.1 假如该分支 节点与Q的距离大于当前最近距离，则继续向上回溯，直至根节点
                3.2 假如该分支节点与Q距离小于当前距离，则使用该分支，走 1 2 步直到叶子节点，更新叶子与Q的最近距离。 
   LSH :
        如果两篇文档相似，那么让他们的hashcode尽可能的相同，即发生碰撞。
        如果两篇文档不相似，那么尽可能的让他们hashcode不相同。
        随机投影法。我们通过随机生成固定数量N的超平面后，根据向量是处在超平面的正面还是负面，将向量hash为Nbit的hashcode
        这一方法符合直觉，如果两个点相似，即距离相近，那么通过随机投影，他们会有较高的概率拥有相同的hashcode，但如果相距较远，他们的拥有相同的hashcode概率也会较低。同时计算一个点在超平面的正面还是负面也相当简单，你只需要对他们做点积即可。
   IVFPQ: 乘积量化（ PQ ） + 倒排  乘积量化（SQ)
        不仅可以减少搜索空间，还能压缩向量存储。
        1. 先使用k-means算法进行聚类：
            - 在数据集随机找k个向量，作为k个聚类的中心
            - 遍历所有的数据节点，计算每个节点跟k个聚类中心的距离，然后将节点归入最近的一类中
            - 针对每个类，统计他下面所有向量的平均值，作为新的聚类中心
            - 重复 2 3 ，直到达到一定次数或者数据集开始收揽，结束流程。
        针对聚类中心建立倒排链。查询的时候，先使用 查询向量Q去跟n个聚类中心算一下最近距离，找到最近的聚类中心，遍历下Q与聚类中心的所有节点，取出topk返回。
        乘积量化便是巧妙的使用聚类ID来表达向量的方式，来达到压缩的目的。

        建立过程：
            1. 使用k-means聚类，将所有向量分为1024个聚类。以聚类ID为key建立倒排
            2. 每个聚类内的样本向量，计算他们与聚类中心的差值，得到新向量。
            3. 使用乘积量化，存储压缩每个聚类的新的样本向量。
        查询过程：
            1. 先计算离哪个聚心近，然后查倒排表取出所有向量。
            2. 计算查询向量与聚类中心的差值，得到新的查询向量。
            3. 使用量化乘积，便利该聚类所有的压缩向量，去最近的k个结果。

            倒排索引 聚类中心 乘积量化码本 量化索引。 PQ是用来向量压缩， iVF减少检索空间。
        https://colab.research.google.com/drive/1xD0aTxnTnHcA8I9bU8qQcevji4z5qj8O?usp=sharing#scrollTo=HkAnk4XQj4_b
        https://zhuanlan.zhihu.com/p/378725270

        一般我们将不会在样本Y上直接做PQ量化，而是对Y和q(Y)的残差向量(向量减法，Y-q(Y))做量化。

   HNSW： 
    1. 所有节点必须联通， 联通图
    2.每个节点有固定的m个友点
    3. 有高速公路机制。
    4. 相近的点互相连接。
     NSW
        NSW朴素构图算法在这里：向图中逐个插入点，插图一个全新点时，通过朴素想法中的朴素查找法（通过计算“友点”和待插入点的距离来判断下一个进入点是哪个点）查找到与这个全新点最近的m个点（m由用户设置），连接全新点到m个点的连线。
        缺点：
            搜索时，NSW无法区分long range与short range，从而无法先查询long range再查询short range。
            当数据的聚类效应特别明显时，即使我们乱序加入向量，cluster之间相互连接的边仍然十分稀疏，从而搜索结果容易陷入局部最优，同时效率也会比较低下。
    HNSW 分层：
        首先会通过一个指数衰减的概率函数，得到这个向量所处的最大层级 类似跳表。然后贪心找到每层最近的点，向下层搜索
        我们会选择与inserted之间的距离相比与其他result更近的点，而不是距离inserted更近的点
        
    各种算法的评估：
        性能 及 召回率两个方面
          在召回topk个情况下，实际上数据集的k 近邻居为 N，而通过Ann近似搜索的k近邻为M，两者取交集的个数 再处于 k个。召回率越高，表示ann算法近似效果越好。
               ( N 交 M) / k 
        但大部分情况下，召回率越高，qps或者耗时情况越差。
## 向量索引如何设计增加实时
## 向量索引如何检索效率优化 及构建效率优化?
## 向量索引如何压缩？
## 向量索引如何借助GPU加速
## 


## 向量检索相关名词：
  **向量模计算公式** （x,y） sqrt(x*x + y*y)
  **向量归一化** ： 将向量的长度（或大小）调整为1（单位向量），同时保持其方向不变。 计算方法 (x, y)  (x/模长， y/模长)
  **ANN** : 近似向量最近邻检索
  **方差** ： （a,b,c) x为平均值  (a-x)* (a-x) + (b-x) * (b-x) + (c-x) * (c-x)|3
  **标准方差** ：方差开根